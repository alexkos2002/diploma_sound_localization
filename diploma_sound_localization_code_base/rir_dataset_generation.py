# -*- coding: utf-8 -*-
"""RIR_dataset_generation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p7BOSA7BfT3LHNBNuDS_EWj8HKbSTUr7
"""

!pip install pyroomacoustics

!pip install sounddevice

!sudo apt-get install libportaudio2

import numpy as np
import matplotlib.pyplot as plt
import pyroomacoustics as pra
import sounddevice as snd
import argparse
from scipy.io import wavfile
import os
import csv

"""**Arctic dataset**"""

# Here, the corpus for speaker bdl is automatically downloaded
# if it is not available already
corpus = pra.datasets.CMUArcticCorpus(download=True, speaker=['bdl'])

# print dataset info and 10 sentences
print(corpus)
corpus.head(n=10)

# let's extract all samples containing the word 'what'
keyword = 'what'
matches = corpus.filter(text=lambda t : keyword in t)
print('The number of sentences containing "{}": {}'.format(keyword, len(matches)))
for s in matches.sentences:
    print('  *', s)

# if the sounddevice package is available, we can play the sample
#matches[1].play()

audio_signal = matches[1]
audio_signal_samples = audio_signal.data
audio_signal_size = len(audio_signal_samples)

print("\nLength(number of samples) of audio signal: %d" % audio_signal_size)
print("Audio signal sampling frequency: %d" % audio_signal.fs)
print("Type of audio signal representation: %s" % (type(audio_signal)))
print("Audio signal in time domain: ")
plt.plot(np.arange(0, audio_signal_size), audio_signal.data)


# show the spectrogram
plt.figure()
audio_signal.plot()
plt.show()

"""**Google Speech Commands dataset**"""

google_speech_dataset = pra.datasets.GoogleSpeechCommands(download=True, subset=10, seed=0)

# print dataset info, first 10 entries, and all sounds
print(google_speech_dataset)
google_speech_dataset.head(n=10)
print("All sounds in the google_speech_dataset:")
print(google_speech_dataset.classes)

# filter by specific word
selected_word = 'yes'
matches = google_speech_dataset.filter(word=selected_word)
print("Number of '%s' samples : %d" % (selected_word, len(matches)))

# if the sounddevice package is available, we can play the sample
#matches[0].play()

audio_signal = matches[0]
audio_signal_samples = audio_signal.data
audio_signal_size = len(audio_signal_samples)

print("Length(number of samples) of audio signal: %d" % audio_signal_size)
print("Audio signal sampling frequency: %d" % audio_signal.fs)
print("Type of audio signal representation: %s" % (type(audio_signal)))
print("Audio signal in time domain: ")
plt.plot(np.arange(0, audio_signal_size), audio_signal.data)

# show the spectrogram
plt.figure()
audio_signal.plot()
plt.show()

"""**Generating RIR dataset from 'yes' word recordings from Google Speech Commands Dataset.**"""

def gen_rect_coords_by_polar_coords(angles, distances):
    points_x_coords = np.outer(distances.reshape(distances.shape[0], 1), np.cos(angles.reshape(1, angles.shape[0])))
    points_y_coords = np.outer(distances.reshape(distances.shape[0], 1), np.sin(angles.reshape(1, angles.shape[0])))
    points_x_coords = points_x_coords.reshape(-1)
    points_y_coords = points_y_coords.reshape(-1)
    return points_x_coords.reshape(-1),  points_y_coords.reshape(-1)

def gen_2d_points_by_polar_coords(angles, distances):
  points_x_coords = np.outer(distances.reshape(distances.shape[0], 1), np.cos(angles.reshape(1, angles.shape[0])))
  points_y_coords = np.outer(distances.reshape(distances.shape[0], 1), np.sin(angles.reshape(1, angles.shape[0])))
  points_x_coords, points_y_coords = gen_rect_coords_by_polar_coords(angles, distances)
  return np.column_stack((points_x_coords, points_y_coords))
  #print(points_x_coords)
  #print(points_y_coords)

def get_angle_and_distance_by_2d_point_index(point_index, angles, distances):
  return angles[point_index % len(angles)], distances[point_index // len(angles)]

def write_dict_to_csv(d, file_path):
  keys = sorted(d.keys())
  with open(file_path, "w") as outfile:
    writer = csv.writer(outfile, delimiter = ",")
    writer.writerow(keys)
    writer.writerows(zip(*[d[key] for key in keys]))

def get_hash_from_google_command_audio_file_name(file_name):
  return file_name[0:8]

yes_speech_com_audio_files_dir_path = "/content/google_speech_commands/yes/"
yes_speech_com_audio_files_pathes = os.listdir(yes_speech_com_audio_files_dir_path)

sound_localization_data_set_path = "/content/drive/MyDrive/diploma_sound_localization_data/"
sources_params_file_name = "sources_params.csv"
mics_params_file_name = "microphones_params.csv"

print(yes_speech_com_audio_files_pathes)
print("Total amount 'yes' word audio recodings: %d" % len(yes_speech_com_audio_files_pathes))

fs = 16000

room_dim = [6, 6]
room_center = [dim / 2 for dim in room_dim] 

audio_sources_angle_delta = 45
audio_sources_angles_num = int(360 / audio_sources_angle_delta)
audio_sources_angles = np.linspace(0, ((audio_sources_angles_num - 1) / (audio_sources_angles_num / 2)) * np.pi, audio_sources_angles_num)
audio_sources_distances = np.array([2])

# obtain 2D coordinates for each audio source location point
points_x_coords, points_y_coords = gen_rect_coords_by_polar_coords(audio_sources_angles, audio_sources_distances)

# define the locations of the audio sources
audio_sources_loc_points = gen_2d_points_by_polar_coords(audio_sources_angles, audio_sources_distances) + np.array(room_center).reshape(1, 2)

microphones_num = 3
inter_microphone_distance = 0.05

# define the locations of the microphones
mics_locs = pra.beamforming.circular_2D_array(room_center, microphones_num,np.pi / 4, 0.05)

# The desired reverberation time
rt60 = 0.5  # seconds

# High, medium and low SNRs
SNRs = [60, 30, 10]

mics_params = {"m_num": [i for i in range(microphones_num)],
                      "x": (mics_locs[0]).tolist(),
                      "y": (mics_locs[1]).tolist()}
sources_params = {"s_num": [i for i in range(len(audio_sources_angles) * len(audio_sources_distances))], 
                  "x": points_x_coords.tolist(),
                  "y": points_y_coords.tolist(),
                  "angle": audio_sources_angles.tolist() * len(audio_sources_distances),
                  "distance": (np.repeat(audio_sources_distances, len(audio_sources_angles))).tolist()}

write_dict_to_csv(sources_params, f"{sound_localization_data_set_path}{sources_params_file_name}")
write_dict_to_csv(mics_params, f"{sound_localization_data_set_path}{mics_params_file_name}")

plt.title("Circular microphone array")
plt.plot(mics_locs[0], mics_locs[1], "*")
plt.plot(room_center[0], room_center[1], "o")

# We invert Sabine's formula to obtain the parameters for the ISM simulator
e_absorption, max_order = pra.inverse_sabine(rt60, room_dim)

for audio_file_name in yes_speech_com_audio_files_pathes[:2]:
  # import a mono wavfile as the source signal
  # the sampling frequency should match that of the room
  audio_signal_fs, audio_signal = wavfile.read(yes_speech_com_audio_files_dir_path + audio_file_name)
  
  for i in range(len(audio_sources_loc_points)):
    audio_source_loc_point = audio_sources_loc_points[i]
    for SNR in SNRs:
    # Create the room
      room = pra.ShoeBox(
          room_dim, fs=audio_signal_fs, materials=pra.Material(e_absorption), max_order=max_order
      )
      # place the source in the room
      # place the microphone array in the room
      room.add_microphone_array(mics_locs)
      room.add_source(audio_source_loc_point.tolist(), signal=audio_signal, delay=0.0)

      # Run the simulation (this will also build the RIR automatically)
      room.simulate(reference_mic=1, snr=SNR)

      # measure the reverberation time
      rt60_sim = room.measure_rt60()
      print("The desired RT60 was {}".format(rt60))
      print("The measured RT60 is {}".format(rt60_sim[1, 0]))

      f = plt.figure()
      f.set_figwidth(12)
      f.set_figheight(15)

      audio_file_hash = get_hash_from_google_command_audio_file_name(audio_file_name)
      os.mkdir(f"{sound_localization_data_set_path}{audio_file_hash}_source_{i}_rt60_{rt60}_SNR_{SNR}")

      for j in range(microphones_num):
        
        # plot RIR for each microphone in microphone array. both can also be plotted using room.plot_rir()`
        rir_1_0 = room.rir[j][0]
        plt.subplot(microphones_num * 2, 1, 2 * j + 1)
        plt.plot(np.arange(len(rir_1_0)) / room.fs, rir_1_0)
        plt.title("The RIR from source with position (%f, %f) to mic %d and SNR %d db" % (j, audio_source_loc_point[0], audio_source_loc_point[1], SNR))
        plt.xlabel("Time [s]")

        #get audio signal for each microphone in microphone array
        #then convert signal from float64 to int16(2**16 quants) and normalize it
        cur_mic_audio_signal = room.mic_array.signals[j, :]
        max_amplitude = np.max(cur_mic_audio_signal)
        cur_mic_audio_signal = (room.mic_array.signals[j, :] / max_amplitude * 2**15).astype(np.int16)

        # plot audio signal for each microphone in microphone array
        plt.subplot(microphones_num * 2, 1, 2 * j + 2)

        plt.plot(cur_mic_audio_signal)
        plt.title("Signal on microphone %d with source position (%f, %f) and SNR %d db:" % (j, audio_source_loc_point[0], audio_source_loc_point[1], SNR))
        plt.xlabel("Time [s]")

        #audio_source_angle, audio_source_distance = get_angle_and_distance_by_2d_point_index(i, audio_sources_angles, audio_sources_distances)

        print(type(cur_mic_audio_signal[0]))

        wavfile.write(f'{sound_localization_data_set_path}{audio_file_hash}_source_{i}_rt60_{rt60}_SNR_{SNR}/{audio_file_hash}_source_{i}_mic_{j}_rt60_{rt60}_SNR_{SNR}.wav', fs, cur_mic_audio_signal)
        #wavfile.read(f'/content/drive/MyDrive/diploma_sound_localization_data/yes_speech_com_mic1_reverb_rt60_{rt60}_SNR_{SNR}db.wav')

        plt.tight_layout()

      plt.show()

